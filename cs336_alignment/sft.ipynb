{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e69c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 09:36:29 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import Callable, List\n",
    "from cs336_alignment.drgrpo_grader import r1_zero_reward_fn\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from cs336_alignment.sft import *\n",
    "import wandb\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_id = \"Qwen/Qwen2.5-Math-1.5B\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sampling_params: SamplingParams = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=1024,\n",
    "        stop=[\"</answer>\"],\n",
    "        include_stop_str_in_output=True,\n",
    "    )\n",
    "    prompt_template: str = \"\"\n",
    "    train_data: str = \"results/math_1.5B_train.jsonl\"\n",
    "    eval_data: str = \"jeggers/competition_math\"\n",
    "    device: str = \"cuda\"\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    eval_gap = 16\n",
    "    train_reader_local_batch_size: int = 4\n",
    "    eval_reader_local_batch_size: int = 32\n",
    "    n_epochs: int = 3\n",
    "    n_steps_per_epoch: int = 78\n",
    "    peak_lr: float = 2e-5\n",
    "    total_steps = 1000\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    expert_iteration: bool = True\n",
    "    G = 4\n",
    "    D_b = 512\n",
    "    vllm_gpu_memory_utilization = 0.1\n",
    "    n_ei_steps = 5\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "with open(\"prompts/r1_zero.prompt\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "\n",
    "def init_policy_model(\n",
    "    model_id: str,\n",
    "    device: str,\n",
    "):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_policy_into_vllm_instance(policy: PreTrainedModel, llm: LLM):\n",
    "    state_dict = policy.state_dict()\n",
    "    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "    llm_model.load_weights(state_dict.items())\n",
    "\n",
    "\n",
    "def evaluate_vllm(\n",
    "    vllm_model: LLM,\n",
    "    reward_fn: Callable[[str, str], dict[str, float]],\n",
    "    prompts: List[str],\n",
    "    ground_truth_list: List[str],\n",
    "    eval_sampling_params: SamplingParams,\n",
    "):\n",
    "    res = []\n",
    "    assert len(prompts) == 512\n",
    "    model_outputs = vllm_model.generate(prompts, eval_sampling_params)\n",
    "    for (\n",
    "        prompt,\n",
    "        output,\n",
    "        ground_truth,\n",
    "    ) in zip(prompts, model_outputs, ground_truth_list):\n",
    "        for completition in output.outputs:\n",
    "            model_answer = completition.text\n",
    "            res.append(\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"generated_text\": model_answer,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"score\": reward_fn(model_answer, ground_truth),\n",
    "                }\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def run_eval(model, eval_model, eval_loader):\n",
    "    examples = next(iter(eval_loader))\n",
    "    # examples = dataset[\"test\"][i : i + eval_batch_size]\n",
    "    prompts = [\n",
    "        prompt_template.format(question=problem) for problem in examples[\"problem\"]\n",
    "    ]\n",
    "    extracted_solutions = [solution for solution in examples[\"extracted_solution\"]]\n",
    "    batch_results = evaluate_vllm(\n",
    "        eval_model, r1_zero_reward_fn, prompts, extracted_solutions, cfg.sampling_params\n",
    "    )\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "# # Setup wandb metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad424d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# wandb.init(project=\"math-sft\", config=cfg)\n",
    "# wandb.define_metric(\"train_step\")\n",
    "# wandb.define_metric(\"eval_step\")\n",
    "# wandb.define_metric(\"train/*\", step_metric=\"train_step\")\n",
    "# wandb.define_metric(\"eval/*\", step_metric=\"eval_step\")\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=cfg.train_data, split=\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.train_reader_local_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "eval_dataset = load_dataset(cfg.eval_data, \"original\", split=\"test\")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=cfg.eval_reader_local_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "try:\n",
    "    del model\n",
    "    del eval_model\n",
    "except:\n",
    "    pass\n",
    "model = init_policy_model(cfg.model_id, cfg.device)\n",
    "eval_model = init_vllm(cfg.model_id, cfg.device, seed=42, gpu_memory_utilization=0.08)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.peak_lr,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    fused=True,\n",
    ")\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=cfg.n_epochs * cfg.n_steps_per_epoch,\n",
    ")\n",
    "\n",
    "global_train_step = 0\n",
    "global_eval_step = 0\n",
    "\n",
    "for epoch in range(cfg.n_epochs):\n",
    "    print(f\"=== Starting Epoch {epoch + 1} / {cfg.n_epochs} ===\")\n",
    "    for idx, examples in enumerate(train_loader):\n",
    "        prompt_strs = examples[\"prompt\"]\n",
    "        output_strs = examples[\"generated_text\"]\n",
    "        model_input = tokenize_prompt_and_output(\n",
    "            prompt_strs, output_strs, tokenizer, cfg.device\n",
    "        )\n",
    "        model_output = get_response_log_probs(\n",
    "            model, model_input[\"input_ids\"], model_input[\"labels\"], True\n",
    "        )\n",
    "        loss, metadata = sft_microbatch_train_step(\n",
    "            model_output[\"log_probs\"],\n",
    "            model_input[\"response_mask\"],\n",
    "            cfg.gradient_accumulation_steps,\n",
    "            1.0,\n",
    "        )\n",
    "\n",
    "        if (idx + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_train_step += 1\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_step\": global_train_step,\n",
    "                    \"train/loss\": loss.item() * cfg.gradient_accumulation_steps,\n",
    "                    \"train/grad_norm\": grad_norm.item(),\n",
    "                    \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"train/token_entropy\": torch.mean(\n",
    "                        model_output[\"token_entropy\"]\n",
    "                    ).item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Eval after each update!\n",
    "            # if idx % cfg.eval_gap == 0:\n",
    "            global_eval_step += 1\n",
    "            eval_res = run_eval(model, eval_model, eval_loader)\n",
    "            avg_format_reward = np.mean([x[\"score\"][\"format_reward\"] for x in eval_res])\n",
    "            avg_answer_reward = np.mean([x[\"score\"][\"answer_reward\"] for x in eval_res])\n",
    "            table = wandb.Table(\n",
    "                columns=[\"prompt\", \"generated_text\", \"ground_truth\", \"score\"],\n",
    "                data=pd.DataFrame(eval_res).astype(str),\n",
    "            )\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"eval_step\": global_eval_step,\n",
    "                    \"eval/format_reward\": avg_format_reward,\n",
    "                    \"eval/answer_reward\": avg_answer_reward,\n",
    "                    \"eval/examples\": table,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab5f75",
   "metadata": {},
   "source": [
    "# Expert Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426402de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 09:36:42 config.py:542] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 11-23 09:36:42 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-23 09:36:43 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 11-23 09:36:43 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-Math-1.5B...\n",
      "INFO 11-23 09:36:43 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 11-23 09:36:44 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac6a9169437429a9df223e28702ab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 09:36:45 model_runner.py:1115] Loading model weights took 2.8797 GB\n",
      "INFO 11-23 09:36:45 worker.py:267] Memory profiling takes 0.52 seconds\n",
      "INFO 11-23 09:36:45 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.10) = 7.93GiB\n",
      "INFO 11-23 09:36:45 worker.py:267] model weights take 2.88GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 3.56GiB.\n",
      "INFO 11-23 09:36:46 executor_base.py:110] # CUDA blocks: 8324, # CPU blocks: 9362\n",
      "INFO 11-23 09:36:46 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 32.52x\n",
      "INFO 11-23 09:36:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 09:37:03 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.21 GiB\n",
      "INFO 11-23 09:37:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 18.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:  25%|██▌       | 512/2048 [01:26<04:19,  5.91it/s, est. speed input: 1002.82 toks/s, output: 8885.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Epoch 1 / 3 ===\n",
      "=== Starting Epoch 2 / 3 ===\n",
      "=== Starting Epoch 3 / 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts:  17%|█▋        | 353/2048 [01:03<03:42,  7.61it/s, est. speed input: 908.87 toks/s, output: 7166.96 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.n_ei_steps):\n\u001b[32m    180\u001b[39m     load_policy_into_vllm_instance(model, eval_model)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     train_dataloader = \u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     train(cfg, model, eval_model, train_dataloader, eval_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mget_train_dataloader\u001b[39m\u001b[34m(cfg, eval_model)\u001b[39m\n\u001b[32m     58\u001b[39m rollout_sampling_params: SamplingParams = SamplingParams(\n\u001b[32m     59\u001b[39m     temperature=\u001b[32m1.0\u001b[39m,\n\u001b[32m     60\u001b[39m     top_p=\u001b[32m1.0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     n=cfg.G,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m prompts = [\n\u001b[32m     68\u001b[39m     prompt_template.format(question=problem) \u001b[38;5;28;01mfor\u001b[39;00m problem \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[33m\"\u001b[39m\u001b[33mproblem\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     69\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m rollout_results = \u001b[43mevaluate_vllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr1_zero_reward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mextracted_solution\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrollout_sampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m correct = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m rollout_results \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m1.0\u001b[39m]\n\u001b[32m     79\u001b[39m dataset = SFTDataset(correct)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mevaluate_vllm\u001b[39m\u001b[34m(vllm_model, reward_fn, prompts, ground_truth_list, eval_sampling_params)\u001b[39m\n\u001b[32m     77\u001b[39m res = []\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompts) == \u001b[32m512\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m model_outputs = \u001b[43mvllm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[32m     81\u001b[39m     prompt,\n\u001b[32m     82\u001b[39m     output,\n\u001b[32m     83\u001b[39m     ground_truth,\n\u001b[32m     84\u001b[39m ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, model_outputs, ground_truth_list):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m completition \u001b[38;5;129;01min\u001b[39;00m output.outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/utils.py:1086\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1081\u001b[39m         warnings.warn(\n\u001b[32m   1082\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1083\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1084\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:469\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    459\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    462\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    463\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m     guided_options=guided_options_request,\n\u001b[32m    467\u001b[39m     priority=priority)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1390\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1388\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1392\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:1386\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[32m   1383\u001b[39m     execute_model_req.async_callback = \u001b[38;5;28mself\u001b[39m.async_callbacks[\n\u001b[32m   1384\u001b[39m         virtual_engine]\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scheduler_config.is_multi_step:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:138\u001b[39m, in \u001b[36mExecutorBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[32m    137\u001b[39m ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:51\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     50\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/utils.py:2220\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2219\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py:389\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes at least one model step on the given sequences, unless no\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[33;03msequences are provided.\"\"\"\u001b[39;00m\n\u001b[32m    387\u001b[39m start_time = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py:374\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.prepare_input\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    372\u001b[39m             broadcast_tensor_dict({}, src=\u001b[32m0\u001b[39m)\n\u001b[32m    373\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_driver_input_and_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_worker_input_from_broadcast()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py:336\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase._get_driver_input_and_broadcast\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_driver_worker\n\u001b[32m    333\u001b[39m worker_input: WorkerInput = \u001b[38;5;28mself\u001b[39m.prepare_worker_input(\n\u001b[32m    334\u001b[39m     execute_model_req=execute_model_req)\n\u001b[32m    335\u001b[39m model_input: ModelRunnerInputBase = (\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_model_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinished_requests_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    341\u001b[39m kwargs = extract_previous_hidden_states(execute_model_req)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_metadata_broadcast:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1628\u001b[39m, in \u001b[36mModelRunner.prepare_model_input\u001b[39m\u001b[34m(self, seq_group_metadata_list, virtual_engine, finished_requests_ids)\u001b[39m\n\u001b[32m   1609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_model_input\u001b[39m(\n\u001b[32m   1610\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1611\u001b[39m     seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[32m   1612\u001b[39m     virtual_engine: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1613\u001b[39m     finished_requests_ids: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1614\u001b[39m ) -> ModelInputForGPUWithSamplingMetadata:\n\u001b[32m   1615\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare the model input based on a given sequence group, including\u001b[39;00m\n\u001b[32m   1616\u001b[39m \u001b[33;03m    metadata for the sampling step.\u001b[39;00m\n\u001b[32m   1617\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1626\u001b[39m \u001b[33;03m    If cuda graph is required, this API automatically pads inputs.\u001b[39;00m\n\u001b[32m   1627\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1628\u001b[39m     model_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_model_input_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinished_requests_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m   1631\u001b[39m         \u001b[38;5;66;03m# Sampling metadata is only required for the final pp group\u001b[39;00m\n\u001b[32m   1632\u001b[39m         generators = \u001b[38;5;28mself\u001b[39m.get_generators(finished_requests_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1220\u001b[39m, in \u001b[36mGPUModelRunnerBase._prepare_model_input_tensors\u001b[39m\u001b[34m(self, seq_group_metadata_list, finished_requests_ids)\u001b[39m\n\u001b[32m   1216\u001b[39m     \u001b[38;5;28mself\u001b[39m.builder.add_seq_group(seq_group_metadata)\n\u001b[32m   1218\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.reset_cached_inter_data()\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:925\u001b[39m, in \u001b[36mModelInputForGPUBuilder.build\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    922\u001b[39m     seq_lens.extend(itertools.repeat(\u001b[32m1\u001b[39m, cuda_graph_pad_size))\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# Attention metadata.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m attn_metadata = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_metadata_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_graph_pad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# LoRA data.\u001b[39;00m\n\u001b[32m    929\u001b[39m lora_requests = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:530\u001b[39m, in \u001b[36mFlashAttentionMetadataBuilder.build\u001b[39m\u001b[34m(self, seq_lens, query_lens, cuda_graph_pad_size, batch_size)\u001b[39m\n\u001b[32m    528\u001b[39m     \u001b[38;5;28mself\u001b[39m.block_tables.extend([] * cuda_graph_pad_size)\n\u001b[32m    529\u001b[39m     num_decode_tokens = batch_size - \u001b[38;5;28mself\u001b[39m.num_prefill_tokens\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     block_tables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_graph_runner_block_tables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    533\u001b[39m     block_tables = make_tensor_with_pad(\n\u001b[32m    534\u001b[39m         \u001b[38;5;28mself\u001b[39m.block_tables,\n\u001b[32m    535\u001b[39m         pad=\u001b[32m0\u001b[39m,\n\u001b[32m    536\u001b[39m         dtype=torch.int,\n\u001b[32m    537\u001b[39m         device=device,\n\u001b[32m    538\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment5-alignment/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:479\u001b[39m, in \u001b[36mFlashAttentionMetadataBuilder._get_graph_runner_block_tables\u001b[39m\u001b[34m(self, num_seqs, block_tables)\u001b[39m\n\u001b[32m    477\u001b[39m num_blocks = \u001b[38;5;28mlen\u001b[39m(block_table)\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_blocks <= max_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     graph_block_tables[i, :num_blocks] = block_table\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# It may be possible to have more blocks allocated due\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# to lookahead slots of multi-step, however, they are\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;66;03m# not used anyway, so can be safely ignored.\u001b[39;00m\n\u001b[32m    484\u001b[39m     graph_block_tables[\n\u001b[32m    485\u001b[39m         i, :max_blocks] = block_table[:max_blocks]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sft import init_vllm\n",
    "\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        return {\"prompt\": item[\"prompt\"], \"generated_text\": item[\"generated_text\"]}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    prompts = [b[\"prompt\"] for b in batch]\n",
    "    generated_text = [b[\"generated_text\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompts,  # List[str] or Tokenized Tensors\n",
    "        \"generated_text\": generated_text,\n",
    "    }\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    wandb.init(project=\"math-sft\", config=cfg)\n",
    "    wandb.define_metric(\"train_step\")\n",
    "    wandb.define_metric(\"eval_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"train_step\")\n",
    "    wandb.define_metric(\"eval/*\", step_metric=\"eval_step\")\n",
    "    del model\n",
    "    del eval_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = init_policy_model(cfg.model_id, cfg.device)\n",
    "eval_model = init_vllm(\n",
    "    cfg.model_id,\n",
    "    cfg.device,\n",
    "    seed=42,\n",
    "    gpu_memory_utilization=cfg.vllm_gpu_memory_utilization,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id)\n",
    "\n",
    "\n",
    "def get_train_dataloader(cfg, eval_model):\n",
    "    rollout_dataset = load_dataset(cfg.eval_data, \"original\", split=\"train\")\n",
    "    rollout_loader = torch.utils.data.DataLoader(\n",
    "        rollout_dataset,\n",
    "        batch_size=cfg.D_b,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    examples = next(iter(rollout_loader))\n",
    "\n",
    "    rollout_sampling_params: SamplingParams = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=1024,\n",
    "        min_tokens=4,\n",
    "        stop=[\"</answer>\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=cfg.G,\n",
    "    )\n",
    "    prompts = [\n",
    "        prompt_template.format(question=problem) for problem in examples[\"problem\"]\n",
    "    ]\n",
    "\n",
    "    rollout_results = evaluate_vllm(\n",
    "        eval_model,\n",
    "        r1_zero_reward_fn,\n",
    "        prompts,\n",
    "        examples[\"extracted_solution\"],\n",
    "        rollout_sampling_params,\n",
    "    )\n",
    "    correct = [x for x in rollout_results if x[\"score\"][\"reward\"] == 1.0]\n",
    "    dataset = SFTDataset(correct)\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.train_reader_local_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,  # Critical for variable length sequences\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "eval_dataset = load_dataset(cfg.eval_data, \"original\", split=\"test\")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=cfg.eval_reader_local_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.peak_lr,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    fused=True,\n",
    ")\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=cfg.n_epochs * cfg.n_steps_per_epoch,\n",
    ")\n",
    "\n",
    "\n",
    "def train(cfg, model, eval_model, train_loader, eval_loader):\n",
    "    global_train_step = 0\n",
    "    global_eval_step = 0\n",
    "\n",
    "    for epoch in range(cfg.n_epochs):\n",
    "        print(f\"=== Starting Epoch {epoch + 1} / {cfg.n_epochs} ===\")\n",
    "        for idx, examples in enumerate(train_loader):\n",
    "            prompt_strs = examples[\"prompt\"]\n",
    "            output_strs = examples[\"generated_text\"]\n",
    "            model_input = tokenize_prompt_and_output(\n",
    "                prompt_strs, output_strs, tokenizer, cfg.device\n",
    "            )\n",
    "            model_output = get_response_log_probs(\n",
    "                model, model_input[\"input_ids\"], model_input[\"labels\"], True\n",
    "            )\n",
    "            loss, metadata = sft_microbatch_train_step(\n",
    "                model_output[\"log_probs\"],\n",
    "                model_input[\"response_mask\"],\n",
    "                cfg.gradient_accumulation_steps,\n",
    "                1.0,\n",
    "            )\n",
    "\n",
    "            if (idx + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1.0\n",
    "                )\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_train_step += 1\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train_step\": global_train_step,\n",
    "                        \"train/loss\": loss.item() * cfg.gradient_accumulation_steps,\n",
    "                        \"train/grad_norm\": grad_norm.item(),\n",
    "                        \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"train/token_entropy\": torch.mean(\n",
    "                            model_output[\"token_entropy\"]\n",
    "                        ).item(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Eval after each update!\n",
    "                # if idx % cfg.eval_gap == 0:\n",
    "                global_eval_step += 1\n",
    "                load_policy_into_vllm_instance(model, eval_model)\n",
    "                eval_res = run_eval(model, eval_model, eval_loader)\n",
    "                avg_format_reward = np.mean(\n",
    "                    [x[\"score\"][\"format_reward\"] for x in eval_res]\n",
    "                )\n",
    "                avg_answer_reward = np.mean(\n",
    "                    [x[\"score\"][\"answer_reward\"] for x in eval_res]\n",
    "                )\n",
    "                table = wandb.Table(\n",
    "                    columns=[\"prompt\", \"generated_text\", \"ground_truth\", \"score\"],\n",
    "                    data=pd.DataFrame(eval_res).astype(str),\n",
    "                )\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"eval_step\": global_eval_step,\n",
    "                        \"eval/format_reward\": avg_format_reward,\n",
    "                        \"eval/answer_reward\": avg_answer_reward,\n",
    "                        \"eval/examples\": table,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "for step in range(cfg.n_ei_steps):\n",
    "    load_policy_into_vllm_instance(model, eval_model)\n",
    "    train_dataloader = get_train_dataloader(cfg, eval_model)\n",
    "    train(cfg, model, eval_model, train_dataloader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e998c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
